{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_X_y(path):\n",
    "    folder = listdir(path)\n",
    "    X = []\n",
    "    y = []\n",
    "    for index, file in enumerate(folder):\n",
    "        df = pd.read_csv(f\"{path}/{file}\", header = None)\n",
    "        X.extend(df.values)\n",
    "        y.extend([index]*len(df))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = csv_to_X_y('../test_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming X is your feature matrix and y is the corresponding labels\n",
    "\n",
    "# Multiclass SVM with Gaussian kernel and 10-fold cross-validation\n",
    "svm_classifier = SVC(kernel='rbf', gamma='scale', C=1.0)  # You can adjust kernel parameters as needed\n",
    "\n",
    "# Using stratified K-fold for multiclass classification\n",
    "kf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "svm_scores = cross_val_score(svm_classifier, X, y, cv=kf)\n",
    "print(\"Multiclass SVM Accuracy:\", np.mean(svm_scores))\n",
    "\n",
    "# LDA classifier with Monte-Carlo cross-validation (50 runs)\n",
    "lda_classifier = LinearDiscriminantAnalysis()\n",
    "\n",
    "# Assuming you have multiple runs and want to average the results\n",
    "num_runs = 50\n",
    "lda_accuracies = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    # Split the data into training and testing sets for each run\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=run)\n",
    "\n",
    "    lda_classifier.fit(X_train, y_train)\n",
    "    y_pred = lda_classifier.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    lda_accuracies.append(accuracy)\n",
    "\n",
    "average_lda_accuracy = np.mean(lda_accuracies)\n",
    "print(\"LDA Average Accuracy over 50 runs:\", average_lda_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset as an example\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# Standardize the data (optional, but recommended for PCA)\n",
    "# mean = np.mean(X, axis=0)\n",
    "# std = np.std(X, axis=0)\n",
    "# X_standardized = (X - mean) / std\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Scree plot\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scree plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "\n",
    "# Cumulative explained variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# # Load the Iris dataset as an example\n",
    "# iris = load_iris()\n",
    "# X = iris.data\n",
    "# y = iris.target\n",
    "\n",
    "# Perform LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "X_lda = lda.fit_transform(X, y)\n",
    "\n",
    "# Scree plot\n",
    "explained_variance_ratio = lda.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scree plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Linear Discriminant')\n",
    "plt.ylabel('Variance Explained')\n",
    "\n",
    "# Cumulative explained variance\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.xlabel('Number of Linear Discriminants')\n",
    "plt.ylabel('Cumulative Variance Explained')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = [\"granade\", \"gun 1\", \"gun 2\", \"jump\", \"punch\"]\n",
    "\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(f\"../data/{classes[i]}/005.csv\", header = None)\n",
    "    \n",
    "    x = list(range(len(df)))\n",
    "    for j in range(3):\n",
    "        plt.subplot(5, 1, i + 1)\n",
    "        plt.plot(x, list(df.iloc[:, j]))\n",
    "    plt.title(classes[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "\n",
    "fs = 100\n",
    "\n",
    "def filtered(df):\n",
    "    fft_result = np.fft.fft(df.values)\n",
    "    fft_freq = np.fft.fftfreq(len(fft_result), 1/fs)\n",
    "\n",
    "    # Identify low-frequency components\n",
    "    low_freq_mask = np.abs(fft_freq) <= 4  # Change the threshold as needed\n",
    "    low_freq_components = fft_result.copy()\n",
    "    low_freq_components[~low_freq_mask] = 0\n",
    "\n",
    "    # Inverse FFT to obtain the signal with only low-frequency components\n",
    "    filtered_signal = np.fft.ifft(low_freq_components).real\n",
    "\n",
    "    return filtered_signal\n",
    "\n",
    "def correlation(df, index, rows):\n",
    "    x = filtered(df.iloc[:, index])\n",
    "    y = filtered(df.iloc[:, (index + 1)%rows])\n",
    "    return np.corrcoef(x, y)[0, 1]\n",
    "\n",
    "def features(df):\n",
    "    X = [[\"max m\"], [\"min m\"], [\"imag\"], [\"imag mean\"], [\"100 th percentile\"], [\"iqr\"], [\"range\"], [\"std\"], [\"half mean\"], [\"half median\"], [\"half 70th percentile\"], [\"correl\"]]\n",
    "    for i in range(len(df.T)):\n",
    "        fft_result = np.fft.fft(df.iloc[:, i].values)\n",
    "        fft_freq = np.fft.fftfreq(len(fft_result), 1/fs)\n",
    "\n",
    "        # Identify low-frequency components\n",
    "        low_freq_mask = np.abs(fft_freq) <= 4  # Change the threshold as needed\n",
    "        low_freq_components = fft_result.copy()\n",
    "        low_freq_components[~low_freq_mask] = 0\n",
    "\n",
    "        # Inverse FFT to obtain the signal with only low-frequency components\n",
    "        filtered_signal = np.fft.ifft(low_freq_components).real\n",
    "        \n",
    "        peaks, _ = find_peaks(filtered_signal)\n",
    "        # plt.plot(peaks, filtered_signal[peaks], \"rx\")\n",
    "        valleys, _ = find_peaks(-filtered_signal)\n",
    "        # plt.plot(valleys, filtered_signal[valleys], \"bx\") \n",
    "\n",
    "        dx = peaks[:, np.newaxis] - valleys\n",
    "        dy = filtered_signal[peaks][:, np.newaxis] - filtered_signal[valleys]\n",
    "\n",
    "        df_dx = (dy/dx).flatten()\n",
    "\n",
    "        X[0].append([max(df_dx) if len(df_dx) != 0 else 0])\n",
    "        X[1].append([min(df_dx) if len(df_dx) != 0 else 0])\n",
    "        X[2].append([low_freq_components[1].imag])\n",
    "        X[3].append([(low_freq_components[:3].imag).mean()])\n",
    "        X[4].append([np.percentile(filtered_signal, 100)])\n",
    "        X[5].append([stats.iqr(filtered_signal)])\n",
    "        X[6].append([np.ptp(filtered_signal)])\n",
    "        X[7].append([np.std(filtered_signal)])\n",
    "        X[8].append([np.mean(filtered_signal[:25])])\n",
    "        X[9].append([np.median(filtered_signal[:25])])\n",
    "        X[10].append([np.percentile(filtered_signal[:25], 70)])\n",
    "        X[11].append([correlation(df, i, len(df.T))])\n",
    "\n",
    "    features = []\n",
    "\n",
    "    for axis in X:\n",
    "        features.extend(axis[1:])\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selected Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "\n",
    "def filtered(df):\n",
    "    data = []\n",
    "    imaginary = []\n",
    "    for i in range(3):\n",
    "        fft_result = np.fft.fft(df.iloc[:, i].values)\n",
    "        fft_freq = np.fft.fftfreq(len(fft_result), 0.01)\n",
    "\n",
    "        # Identify low-frequency components\n",
    "        low_freq_mask = np.abs(fft_freq) <= 4  # Change the threshold as needed\n",
    "        low_freq_components = fft_result.copy()\n",
    "        low_freq_components[~low_freq_mask] = 0\n",
    "\n",
    "        # Inverse FFT to obtain the signal with only low-frequency components\n",
    "        filtered_signal = np.fft.ifft(low_freq_components).real\n",
    "        data.append(filtered_signal)\n",
    "        imaginary.append(low_freq_components.imag[:3])\n",
    "    return np.array(data), np.array(imaginary)\n",
    "\n",
    "def slope(data):\n",
    "    peaks, _ = find_peaks(data)\n",
    "    # plt.plot(peaks, filtered_signal[peaks], \"rx\")\n",
    "    valleys, _ = find_peaks(-data)\n",
    "    # plt.plot(valleys, filtered_signal[valleys], \"bx\") \n",
    "\n",
    "    dx = peaks[:, np.newaxis] - valleys\n",
    "    dy = data[peaks][:, np.newaxis] - data[valleys]\n",
    "\n",
    "    return((dy/dx).flatten())\n",
    "\n",
    "def accl_features(df):\n",
    "    filtered_signal, _ = filtered(df)\n",
    "    features = []\n",
    "\n",
    "    df_dx = slope(filtered_signal[1])\n",
    "    features.append([min(df_dx) if len(df_dx) != 0 else 0])\n",
    "\n",
    "    df_dx = slope(filtered_signal[2])\n",
    "    features.append([min(df_dx) if len(df_dx) != 0 else 0])\n",
    "\n",
    "    features.append([np.percentile(filtered_signal[0], 100)])\n",
    "    features.append([np.percentile(filtered_signal[1], 100)])\n",
    "\n",
    "    features.append([stats.iqr(filtered_signal[1])])\n",
    "\n",
    "    features.append([np.mean(filtered_signal[0][:25])])\n",
    "    features.append([np.mean(filtered_signal[1][:25])])\n",
    "\n",
    "    features.append([np.median(filtered_signal[1][:25])])\n",
    "\n",
    "    features.append([np.percentile(filtered_signal[1][:25], 70)])\n",
    "    return features\n",
    "\n",
    "def gyro_features(df):\n",
    "    filtered_signal, imaginary = filtered(df)\n",
    "    features = []\n",
    "\n",
    "    features.append([imaginary[0][1]])\n",
    "    features.append([imaginary[2][1]])\n",
    "\n",
    "    features.append([(imaginary[0]).mean()])\n",
    "    features.append([(imaginary[2]).mean()])\n",
    "\n",
    "    features.append([stats.iqr(filtered_signal[0])])\n",
    "    features.append([stats.iqr(filtered_signal[2])])\n",
    "\n",
    "    features.append([np.ptp(filtered_signal[2])])\n",
    "\n",
    "    features.append([np.std(filtered_signal[2])])\n",
    "\n",
    "    features.append([np.mean(filtered_signal[0][:25])])\n",
    "\n",
    "    features.append([np.median(filtered_signal[0][:25])])\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "\n",
    "folders = listdir(\"../data\")\n",
    "\n",
    "columns = [(5, 55), (20, 70), (10, 60), (15, 65), (5, 55), (5, 55)]\n",
    "\n",
    "for index, folder in enumerate(folders):\n",
    "    open(f\"../test_features/{folder}.csv\", \"w\").close()\n",
    "\n",
    "    files = listdir(f\"../data/{folder}\")[:17]\n",
    "    \n",
    "    for file in files:\n",
    "        data = pd.read_csv(f\"../data/{folder}/{file}\", header=None).iloc[columns[index][0]:columns[index][1], :6]\n",
    "        # feature = features(data.iloc[:, 3:6])\n",
    "        # feature = features(data.iloc[:, 0:3])\n",
    "        # feature.extend(features(data.iloc[:, 3:6]))\n",
    "\n",
    "        feature = accl_features(data.iloc[:, 0:3])\n",
    "        feature.extend(gyro_features(data.iloc[:, 3:6]))\n",
    "\n",
    "        pd.DataFrame(feature).T.to_csv(f\"../test_features/{folder}.csv\", header = None, index = False, mode = \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "folders = listdir(\"../data\")[:-1]\n",
    "columns = [(5, 55), (20, 70), (10, 60), (15, 65), (5, 55)]\n",
    "fs = 100\n",
    "signals = []\n",
    "\n",
    "for folder in folders:\n",
    "    files = listdir(f\"../data/{folder}\")[:17]\n",
    "    signal = [pd.read_csv(f\"../data/{folder}/{file}\") for file in files]\n",
    "    signals.append(signal)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure(figsize = (8,7))\n",
    "    \n",
    "    for j in range(5):\n",
    "        for k in range(3):\n",
    "            signal = signals[j][i].iloc[columns[j][0]:columns[j][1], k].values\n",
    "            # signal -= min(signal)\n",
    "            # signal /= np.ptp(signal)\n",
    "\n",
    "            fft_result = np.fft.fft(signal)\n",
    "            fft_freq = np.fft.fftfreq(len(fft_result), 1/fs)\n",
    "\n",
    "            # Identify low-frequency components\n",
    "            low_freq_mask = np.abs(fft_freq) == 2  # Change the threshold as needed\n",
    "            low_freq_components = fft_result.copy()\n",
    "            low_freq_components[~low_freq_mask] = 0\n",
    "\n",
    "            # Inverse FFT to obtain the signal with only low-frequency components\n",
    "            filtered_signal = np.fft.ifft(low_freq_components).real\n",
    "            plt.subplot(5, 1, j + 1)\n",
    "            # plt.plot(signal, alpha = 0.6)\n",
    "            plt.plot(filtered_signal)\n",
    "\n",
    "            # peaks, _ = find_peaks(filtered_signal, distance = 20, prominence = 0.2)\n",
    "            # plt.plot(peaks, filtered_signal[peaks], \"rx\")\n",
    "            # valleys, _ = find_peaks(-filtered_signal)\n",
    "            # plt.plot(valleys, filtered_signal[valleys], \"bx\")\n",
    "            \n",
    "            # dx = peaks[:, np.newaxis] - valleys\n",
    "            # dy = filtered_signal[peaks][:, np.newaxis] - filtered_signal[valleys]\n",
    "            # # print(max(abs((dy/dx).flatten())))\n",
    "            # # print(dy/dx)\n",
    "            # plt.text(55, (2 - k)*5000, str(max(abs((dy/dx).flatten()))))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from os import listdir\n",
    "from scipy.signal import find_peaks\n",
    "import scipy.stats as stats\n",
    "\n",
    "folders = listdir(\"../data\")[:-1]\n",
    "columns = [(5, 55), (20, 70), (10, 60), (15, 65), (5, 55)]\n",
    "fs = 100\n",
    "signals = []\n",
    "\n",
    "for folder in folders:\n",
    "    files = listdir(f\"../data/{folder}\")[:17]\n",
    "    signal = [pd.read_csv(f\"../data/{folder}/{file}\") for file in files]\n",
    "    signals.append(signal)\n",
    "\n",
    "Mean = []\n",
    "\n",
    "peak = []\n",
    "valley = []\n",
    "dydx = []\n",
    "\n",
    "for i in range(17):\n",
    "    mean1 = []\n",
    "    for j in range(5):\n",
    "        mean2 = []\n",
    "        for k in range(3):\n",
    "            signal = signals[j][i].iloc[columns[j][0]:columns[j][1], k].values\n",
    "            # signal -= min(signal)\n",
    "            # signal /= np.ptp(signal)\n",
    "\n",
    "            fft_result = np.fft.fft(signal)\n",
    "            fft_freq = np.fft.fftfreq(len(fft_result), 1/fs)\n",
    "\n",
    "            # Identify low-frequency components\n",
    "            low_freq_mask = np.abs(fft_freq) <= 4  # Change the threshold as needed\n",
    "            low_freq_components = fft_result.copy()\n",
    "            low_freq_components[~low_freq_mask] = 0\n",
    "\n",
    "            # Inverse FFT to obtain the signal with only low-frequency components\n",
    "            filtered_signal = np.fft.ifft(low_freq_components).real\n",
    "            \n",
    "            peaks, _ = find_peaks(filtered_signal)\n",
    "            # plt.plot(peaks, filtered_signal[peaks], \"rx\")\n",
    "            valleys, _ = find_peaks(-filtered_signal)\n",
    "            # plt.plot(valleys, filtered_signal[valleys], \"bx\")\n",
    "            \n",
    "            peak.append(peaks)\n",
    "            valley.append(valleys)\n",
    "\n",
    "            dx = peaks[:, np.newaxis] - valleys\n",
    "            dy = filtered_signal[peaks][:, np.newaxis] - filtered_signal[valleys]\n",
    "\n",
    "            dydx.append(dy/dx)\n",
    "            \n",
    "            mean2.append([max((dy/dx).flatten())])\n",
    "            # mean2.append([min((dy/dx).flatten())])\n",
    "            # mean2.append([low_freq_components[1].imag])\n",
    "            # mean2.append([(low_freq_components[:3].imag).mean()])\n",
    "            # mean2.append([np.percentile(filtered_signal, 100)])\n",
    "            # mean2.append([stats.iqr(filtered_signal)])\n",
    "            # mean2.append([np.ptp(filtered_signal)])\n",
    "            # mean2.append([np.std(filtered_signal)])\n",
    "            # mean2.append([np.mean(filtered_signal[:25])])\n",
    "            # mean2.append([np.median(filtered_signal[:25])])\n",
    "            # mean2.append([np.percentile(filtered_signal[:25], 70)])\n",
    "        mean1.append(mean2)\n",
    "    Mean.append(mean1)\n",
    "\n",
    "for i in range(3):\n",
    "    for j in range(5):\n",
    "        for k in range(17):\n",
    "            plt.scatter(Mean[k][j][i], j)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## speed graph calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(13, 8))\n",
    "\n",
    "Cm = 0.95\n",
    "g = 9.81\n",
    "M = 200\n",
    "Cwa = 1.2\n",
    "A = 1\n",
    "roh = 1.3\n",
    "P = 1500\n",
    "\n",
    "w = 0/3.6\n",
    "\n",
    "Crs = list(np.linspace(0.001, 0.3, 100))\n",
    "\n",
    "for theta in range(0, 61, 5):\n",
    "    theta = np.deg2rad(theta)\n",
    "    v = []\n",
    "    for Cr in Crs:\n",
    "        Fr = Cr*M*g*np.cos(theta)\n",
    "        Fg = M*g*np.sin(theta)\n",
    "\n",
    "        Fstg = Fr + Fg\n",
    "\n",
    "        a = 1\n",
    "        b = 3*w\n",
    "        c = (b*w) + (2*(Fstg)/Cwa*A*roh)\n",
    "        d = w**3 - (2*P*Cm/Cwa*A*roh)\n",
    "\n",
    "        v.append([max(np.roots([a, b, c, d]).real)*3.6])\n",
    "    plt.plot(Crs, v, label = str(np.round(np.rad2deg(theta), 0)))\n",
    "\n",
    "plt.title(f\"Power {P}W, Headwind speed {w*3.6}Km/hr, Total Mass {M}Kg\")\n",
    "plt.grid(True)\n",
    "plt.legend(title = \"Terrain slope\")\n",
    "plt.yticks(range(0, 51, 5))\n",
    "plt.ylabel(\"velocity (Km/hr)\")\n",
    "plt.xticks(np.round(np.linspace(0.001, 0.3, 15), 3))\n",
    "plt.xlabel(\"Cr\\n0.011 for asphalt, 0.02 for dirt road, 0.3 for farm land\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## power calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cm = 0.95\n",
    "v = 8.2/3.6\n",
    "g = 9.81    \n",
    "Cr = 0.3\n",
    "theta = np.deg2rad(0)\n",
    "M = 200\n",
    "Cwa = 1.2\n",
    "A = 1\n",
    "roh = 1.2\n",
    "w = 10/3.6\n",
    "\n",
    "Fr = Cr*M*g*np.cos(theta)\n",
    "Fg = M*g*np.sin(theta)\n",
    "\n",
    "Fstg = Fr + Fg\n",
    "\n",
    "Pag = 0.5*Cwa*A*roh*(v + w)**3\n",
    "\n",
    "Power = ((Fstg*v) + (Pag))/Cm\n",
    "print(Power)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
